{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame se ha reducido en un 43.42105263157895%\n"
     ]
    }
   ],
   "source": [
    "file_path = 'dataset/n_ladrillera.csv'\n",
    "df = pd.read_csv(file_path, quotechar='\"', delimiter=\";\")\n",
    "df['STOCK_ACTUAL'] = df['STOCK_ACTUAL'].str.replace(',', '.').astype(float)\n",
    "df['TOTAL_CANTIDAD_MOVS'] = df['TOTAL_CANTIDAD_MOVS'].str.replace(',', '.').astype(float)\n",
    "df['PMP'] = df['PMP'].str.replace(',', '.').astype(float)\n",
    "df['TOTAL_IMPORTE_MOVS'] = df['TOTAL_IMPORTE_MOVS'].str.replace(',', '.').astype(float)\n",
    "label_encoder = LabelEncoder()\n",
    "df['ARTICULO'] = label_encoder.fit_transform(df['ARTICULO'])\n",
    "df['TIPO_MATERIAL'] = label_encoder.fit_transform(df['TIPO_MATERIAL'])\n",
    "df['CODIGO_FAMILIA'] = label_encoder.fit_transform(df['CODIGO_FAMILIA'])\n",
    "df.drop(columns=['EMPRESA','PESO_NETO','MULTIPRESENTACION','ARTICULO','CODIGO_FAMILIA'], inplace=True)\n",
    "columns_to_standardize = ['TIPO_MATERIAL','PMP','STOCK_ACTUAL','TOTAL_CANTIDAD_MOVS','TOTAL_IMPORTE_MOVS','TOTAL_MOVS']\n",
    "scaler = StandardScaler()\n",
    "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "# IQR para detectar y eliminar outliers\n",
    "columns = columns_to_standardize\n",
    "\n",
    "filtered_df = df.copy()\n",
    "\n",
    "for column in columns:\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    filtered_df = filtered_df[(filtered_df[column] >= lower_bound) & (filtered_df[column] <= upper_bound)]\n",
    "\n",
    "\n",
    "original_count = df.shape[0]\n",
    "filtered_count = filtered_df.shape[0]\n",
    "reduction = ((original_count - filtered_count) / original_count) * 100\n",
    "\n",
    "print(f\"El DataFrame se ha reducido en un {reduction}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PON AQUÍ TU CÓDIGO\n",
    "# Calcula las correlaciones de Pearson entre todas las variables\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "# Configura el tamaño de la figura\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Crea un mapa de calor de la matriz de correlaciones\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "# Añade título\n",
    "plt.title('Matriz de correlación de Pearson')\n",
    "\n",
    "# Muestra el mapa de calor\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aplicar Clustering Jerárquico\n",
    "agg_model = AgglomerativeClustering(n_clusters=3)\n",
    "y_agg = agg_model.fit_predict(X)\n",
    "\n",
    "# Agregar etiquetas de cluster al dataframe\n",
    "clusters_agg = pd.DataFrame(X, columns=X.columns)\n",
    "clusters_agg['label'] = agg_model.labels_\n",
    "\n",
    "color_map = {0: 'red', 1: 'green', 2: 'blue'}\n",
    "\n",
    "# Visualización polar para Clustering Jerárquico\n",
    "'''\n",
    "polar_agg = clusters_agg.groupby(\"label\").mean().reset_index()\n",
    "polar_agg = pd.melt(polar_agg, id_vars=[\"label\"])\n",
    "fig_agg = px.line_polar(polar_agg, r=\"value\", theta=\"variable\", color=\"label\", line_close=True, height=800, width=900, color_discrete_map=color_map)\n",
    "fig_agg.show()\n",
    "'''\n",
    "\n",
    "# Visualización de distribución de clusters con Clustering Jerárquico\n",
    "pie_agg = clusters_agg.groupby('label').size().reset_index()\n",
    "pie_agg.columns = ['label', 'value']\n",
    "fig_a = px.pie(pie_agg, values='value', names='label', color='label', height=400, width=400, color_discrete_map=color_map,\n",
    "                 labels={\"label\": \"Cluster\", \"value\": \"Número de Instancias\"})\n",
    "fig_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_colors_by_size(labels, color_order):\n",
    "    cluster_sizes = pd.Series(labels).value_counts()\n",
    "    sorted_clusters = cluster_sizes.index\n",
    "    color_map = {sorted_clusters[i]: color_order[i] for i in range(len(sorted_clusters))}\n",
    "    return labels.map(color_map)\n",
    "\n",
    "color_order = ['0', '1', '2']\n",
    "\n",
    "clusters_agg = pd.DataFrame(X, columns=X.columns)\n",
    "clusters_agg['label'] = agg_model.labels_\n",
    "clusters_agg['color_label'] = assign_colors_by_size(clusters_agg['label'], color_order)\n",
    "\n",
    "pie_agg = clusters_agg.groupby('color_label').size().reset_index()\n",
    "pie_agg.columns = ['color_label', 'value']\n",
    "fig_a = px.pie(pie_agg, values='value', names='color_label', height=400, width=400, color='color_label',\n",
    "                 labels={\"color_label\": \"Cluster\", \"value\": \"Número de Instancias\"})\n",
    "\n",
    "fig_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de evaluación\n",
    "silhouette_avg = silhouette_score(X, y_agg)\n",
    "davies_bouldin_avg = davies_bouldin_score(X, y_agg)\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_avg)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansmodel = KMeans(n_clusters = 3, init='k-means++', random_state=0,n_init=10)\n",
    "y_kmeans = kmeansmodel.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters_f = pd.DataFrame(X, columns=X.columns)\n",
    "clusters_f['label'] = kmeansmodel.labels_\n",
    "\n",
    "color_map = {0: 'blue', 1: 'red', 2: 'green'}\n",
    "\n",
    "# Visualización de distribución de clusters con Clustering Jerárquico\n",
    "pie_k = clusters_f.groupby('label').size().reset_index()\n",
    "pie_k.columns = ['label', 'value']\n",
    "fig_k = px.pie(pie_k, values='value', names='label', color='label', height=400, width=400, color_discrete_map=color_map,\n",
    "                 labels={\"label\": \"Cluster\", \"value\": \"Número de Instancias\"})\n",
    "\n",
    "fig_k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de evaluación\n",
    "silhouette_avg = silhouette_score(X, y_kmeans)\n",
    "davies_bouldin_avg = davies_bouldin_score(X, y_kmeans)\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_avg)\n",
    "print(\"Innertia:\",kmeansmodel.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_colors_by_size(labels, color_order):\n",
    "    cluster_sizes = pd.Series(labels).value_counts()\n",
    "    sorted_clusters = cluster_sizes.index\n",
    "    color_map = {sorted_clusters[i]: color_order[i] for i in range(len(sorted_clusters))}\n",
    "    return labels.map(color_map)\n",
    "\n",
    "color_order = ['blue', 'red', 'green']\n",
    "\n",
    "clusters_kmeans = pd.DataFrame(X, columns=X.columns)\n",
    "clusters_kmeans['label'] = kmeansmodel.labels_\n",
    "clusters_kmeans['color_label'] = assign_colors_by_size(clusters_kmeans['label'], color_order)\n",
    "\n",
    "pie_k = clusters_kmeans.groupby('color_label').size().reset_index()\n",
    "pie_k.columns = ['color_label', 'value']\n",
    "fig_k = px.pie(pie_k, values='value', names='color_label', height=400, width=400, color='color_label',\n",
    "                 labels={\"color_label\": \"Cluster\", \"value\": \"Número de Instancias\"})\n",
    "\n",
    "fig_k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "# Crear subplots\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]], subplot_titles=(\"Gráfico tarta Agglomerative Clustering\", \"Gráfico de tarta KMeans\"))\n",
    "\n",
    "# Agregar gráfico polar\n",
    "for trace in fig_a.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "# Agregar gráfico de tarta\n",
    "for trace in fig_k.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "# Actualizar el layout para un título global\n",
    "fig.update_layout(title_text=\"Resultados del Clustering\", title_x=0.5, title_y=0.95, title_font_size=20)\n",
    "\n",
    "# Mostrar la figura\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para DBSCAN\n",
    "eps_values = np.linspace(0.1, 1.0, 10)\n",
    "min_samples_values = range(1, 10)\n",
    "results = []\n",
    "\n",
    "# Calcular el tamaño mínimo del cluster en base al porcentaje\n",
    "min_points_percentage = 0.01  # 5% del total\n",
    "min_points = int(len(X) * min_points_percentage)\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan_model.fit_predict(X)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Excluir ruido\n",
    "        \n",
    "        # Verificar el tamaño mínimo de los clusters\n",
    "        clusters = pd.Series([label for label in labels if label != -1])\n",
    "        if all(clusters.value_counts() >= min_points):\n",
    "            results.append((eps, min_samples, n_clusters))\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'n_clusters'])\n",
    "results_pivot = results_df.pivot_table(values='n_clusters', index='eps', columns='min_samples')\n",
    "\n",
    "# Mostrar la tabla de resultados\n",
    "print(\"Resultados del clustering con DBSCAN (número de clusters):\")\n",
    "print(results_pivot)\n",
    "\n",
    "# Función para encontrar la combinación de parámetros más cercana al número objetivo de clusters\n",
    "def find_best_dbscan_params(target_clusters, results_df):\n",
    "    results_df['diff'] = abs(results_df['n_clusters'] - target_clusters)\n",
    "    best_params = results_df.loc[results_df['diff'].idxmin()]\n",
    "    return best_params['eps'], int(best_params['min_samples'])\n",
    "\n",
    "# Especificar el número objetivo de clusters\n",
    "target_clusters = 3\n",
    "\n",
    "# Encontrar los mejores parámetros\n",
    "best_eps, best_min_samples = find_best_dbscan_params(target_clusters, results_df)\n",
    "print(f\"Mejores parámetros para DBSCAN con objetivo de {target_clusters} clusters: eps={best_eps}, min_samples={best_min_samples}\")\n",
    "\n",
    "# Aplicar DBSCAN con los mejores parámetros encontrados\n",
    "dbscan_model = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "y_dbscan = dbscan_model.fit_predict(X)\n",
    "\n",
    "# Crear un DataFrame para clusters excluyendo el ruido\n",
    "clusters_dbscan = pd.DataFrame(X, columns=X.columns)\n",
    "clusters_dbscan['label'] = y_dbscan\n",
    "\n",
    "# Excluir puntos de ruido\n",
    "clusters_dbscan_valid = clusters_dbscan[clusters_dbscan['label'] != -1]\n",
    "\n",
    "# Calcular las medias de las features para cada cluster\n",
    "cluster_means = clusters_dbscan_valid.groupby('label').mean()\n",
    "\n",
    "# Calcular la media global\n",
    "global_mean = clusters_dbscan_valid.mean()\n",
    "\n",
    "# Comparar las medias de cada cluster con la media global\n",
    "influence = cluster_means - global_mean\n",
    "\n",
    "influence.drop(columns=['label'], inplace=True)\n",
    "\n",
    "# Mostrar la influencia de cada feature en cada cluster\n",
    "print(\"Influencia de las features en cada cluster:\")\n",
    "print(influence)\n",
    "\n",
    "# Visualización de la influencia de las features en cada cluster\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(influence, annot=True, cmap='coolwarm')\n",
    "plt.title('Influencia de las features en cada cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar la influencia de cada feature en cada cluster\n",
    "print(\"Influencia de las features en cada cluster:\")\n",
    "print(influence)\n",
    "\n",
    "# Visualización de la influencia de las features en cada cluster\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(influence, annot=True, cmap='coolwarm')\n",
    "plt.title('Influencia de las features en cada cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de evaluación\n",
    "silhouette_avg = silhouette_score(X, y_dbscan)\n",
    "davies_bouldin_avg = davies_bouldin_score(X, y_dbscan)\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_avg)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suponiendo que X es un DataFrame de pandas y labels son las etiquetas de los clusters obtenidas de cada algoritmo\n",
    "\n",
    "y_dbscan2  = y_dbscan[y_dbscan != -1]\n",
    "\n",
    "Y = X.copy()\n",
    "\n",
    "#quitar de Y los valores -1\n",
    "Y['label'] = y_dbscan\n",
    "Y = Y[Y['label'] != -1]\n",
    "\n",
    "print(Y.shape)\n",
    "print(y_dbscan2.shape)\n",
    "print(X.shape)\n",
    "print(y_dbscan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualización para DBSCAN\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y.iloc[:, 0], Y.iloc[:, 1], c=y_dbscan2, cmap='viridis')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualización para AgglomerativeClustering\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y.iloc[:, 0], X.iloc[:, 1], c=y_agg, cmap='viridis')\n",
    "plt.title('Agglomerative Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# Visualización para KMeans\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y_kmeans, cmap='viridis')\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suponiendo que X es un DataFrame de pandas y labels son las etiquetas de los clusters obtenidas de cada algoritmo\n",
    "\n",
    "# Aplicar PCA para reducir a 2 dimensiones\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "Y_pca = pca.fit_transform(Y)\n",
    "\n",
    "# Visualización para DBSCAN\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_pca[:, 0], Y_pca[:, 1], c=y_dbscan2, cmap='viridis')\n",
    "plt.title('DBSCAN Clustering (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Visualización para AgglomerativeClustering\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_agg, cmap='viridis')\n",
    "plt.title('Agglomerative Clustering (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Visualización para KMeans\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='viridis')\n",
    "plt.title('KMeans Clustering (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters_f = pd.DataFrame(X, columns=X.columns)\n",
    "\n",
    "clusters_f['label'] = y_dbscan\n",
    "clusters_f = clusters_f[clusters_f['label'] != -1]\n",
    "\n",
    "\n",
    "color_map = {0: 'blue', 1: 'red', 2: 'green'}\n",
    "\n",
    "# Visualización de distribución de clusters con Clustering Jerárquico\n",
    "pie_k = clusters_f.groupby('label').size().reset_index()\n",
    "pie_k.columns = ['label', 'value']\n",
    "fig_k = px.pie(pie_k, values='value', names='label', color='label', height=400, width=400, color_discrete_map=color_map,\n",
    "                 labels={\"label\": \"Cluster\", \"value\": \"Número de Instancias\"})\n",
    "\n",
    "fig_k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que X es tu conjunto de datos y ya ha sido definido\n",
    "\n",
    "# Función para asignar colores en base al tamaño del cluster\n",
    "def assign_colors_by_size(labels, color_order):\n",
    "    cluster_sizes = pd.Series(labels).value_counts()\n",
    "    sorted_clusters = cluster_sizes.index\n",
    "    color_map = {sorted_clusters[i]: color_order[i] for i in range(len(sorted_clusters))}\n",
    "    return labels.map(color_map)\n",
    "\n",
    "# Parámetros para DBSCAN\n",
    "eps_values = np.linspace(0.1, 1.0, 10)\n",
    "min_samples_values = range(1, 10)\n",
    "results = []\n",
    "\n",
    "# Calcular el tamaño mínimo del cluster en base al porcentaje\n",
    "min_points_percentage = 0.01  # 1% del total\n",
    "min_points = int(len(X) * min_points_percentage)\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan_model.fit_predict(X)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Excluir ruido\n",
    "        \n",
    "        # Verificar el tamaño mínimo de los clusters\n",
    "        clusters = pd.Series([label for label in labels if label != -1])\n",
    "        if all(clusters.value_counts() >= min_points):\n",
    "            results.append((eps, min_samples, n_clusters))\n",
    "\n",
    "# Convertir los resultados a un DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'n_clusters'])\n",
    "results_pivot = results_df.pivot_table(values='n_clusters', index='eps', columns='min_samples')\n",
    "\n",
    "# Mostrar la tabla de resultados\n",
    "print(\"Resultados del clustering con DBSCAN (número de clusters):\")\n",
    "print(results_pivot)\n",
    "\n",
    "# Función para encontrar la combinación de parámetros más cercana al número objetivo de clusters\n",
    "def find_best_dbscan_params(target_clusters, results_df):\n",
    "    results_df['diff'] = abs(results_df['n_clusters'] - target_clusters)\n",
    "    best_params = results_df.loc[results_df['diff'].idxmin()]\n",
    "    return best_params['eps'], int(best_params['min_samples'])\n",
    "\n",
    "# Especificar el número objetivo de clusters\n",
    "target_clusters = 3\n",
    "\n",
    "# Encontrar los mejores parámetros\n",
    "best_eps, best_min_samples = find_best_dbscan_params(target_clusters, results_df)\n",
    "print(f\"Mejores parámetros para DBSCAN con objetivo de {target_clusters} clusters: eps={best_eps}, min_samples={best_min_samples}\")\n",
    "\n",
    "# Aplicar DBSCAN con los mejores parámetros encontrados\n",
    "dbscan_model = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "y_dbscan = dbscan_model.fit_predict(X)\n",
    "\n",
    "# Crear un DataFrame para clusters excluyendo el ruido\n",
    "clusters_dbscan = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "clusters_dbscan['label'] = y_dbscan\n",
    "\n",
    "# Excluir puntos de ruido\n",
    "clusters_dbscan_valid = clusters_dbscan[clusters_dbscan['label'] != -1].copy()\n",
    "\n",
    "# Asignar colores según el tamaño del cluster\n",
    "color_order = ['blue', 'green', 'red']\n",
    "clusters_dbscan_valid['color_label'] = assign_colors_by_size(clusters_dbscan_valid['label'], color_order)\n",
    "\n",
    "# Calcular las medias de las features para cada cluster\n",
    "cluster_means = clusters_dbscan_valid.groupby('color_label').mean()\n",
    "\n",
    "# Calcular la media global\n",
    "global_mean = clusters_dbscan_valid.mean(numeric_only=True)\n",
    "\n",
    "# Comparar las medias de cada cluster con la media global\n",
    "influence = cluster_means - global_mean\n",
    "\n",
    "# Mostrar la influencia de cada feature en cada cluster\n",
    "print(\"Influencia de las features en cada cluster:\")\n",
    "print(influence)\n",
    "\n",
    "# Visualización de la influencia de las features en cada cluster\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(influence, annot=True, cmap='coolwarm')\n",
    "plt.title('Influencia de las features en cada cluster')\n",
    "plt.show()\n",
    "\n",
    "# Visualización de distribución de clusters con DBSCAN\n",
    "pie_dbscan = clusters_dbscan_valid.groupby('color_label').size().reset_index()\n",
    "pie_dbscan.columns = ['color_label', 'value']\n",
    "fig_pie_dbscan = px.pie(pie_dbscan, values='value', names='color_label', color='color_label', \n",
    "                        color_discrete_map={'blue': 'blue', 'green': 'green', 'red': 'red'},\n",
    "                        labels={\"color_label\": \"Cluster\", \"value\": \"Número de Instancias\"})\n",
    "\n",
    "# Crear subplots para incluir el gráfico de tarta de DBSCAN\n",
    "fig_dbscan = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    subplot_titles=(\"Gráfico de Tarta para Distribución de Clusters DBSCAN\"),\n",
    "    specs=[[{\"type\": \"domain\"}]]\n",
    ")\n",
    "\n",
    "# Agregar gráfico de tarta de DBSCAN\n",
    "for trace in fig_pie_dbscan.data:\n",
    "    fig_dbscan.add_trace(trace, row=1, col=1)\n",
    "\n",
    "# Actualizar el layout para un título global\n",
    "fig_dbscan.update_layout(title_text=\"Resultados del Clustering DBSCAN\")\n",
    "\n",
    "# Mostrar la figura\n",
    "fig_dbscan.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gal_lua",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
